---
title: ""
output:
  html_document:
    df_print: paged
---

<center><img src="https://github.com/Estadistica-AnalisisPolitico/maestriaCpRi/blob/main/POL651.png?raw=true" width="900"/></center>

<br>

<br>

Profesor:[Dr. José Manuel MAGALLANES REYES, Ph.D](http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/" target="_blank) <br>

-   Profesor Principal del Departamento de Ciencias Sociales, Sección de Ciencia Política y Gobierno.

-   [Oficina 223](https://goo.gl/maps/xuGeG6o9di1i1y5m6) - Edificio CISEPA / ECONOMIA / CCSS
-   Telefono: (51) 1 - 6262000 anexo 4302
-   Correo Electrónico: [jmagallanes\@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe)

<a id='beginning'></a>

------------------------------------------------------------------------

<center>

<header>

<h2>Analisis Bivariado</h2>

</header>

</center>



------------------------------------------------------------------------

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

## El *Pipeline* del Analista Político

Podemos proponer que los *analistas* existen para brindar **explicaciones** y **recomendaciones** sobre algún asunto de interés. Por otro lado, el *decisor* es responsable de eligir un curso de acción sabiendo que el analista le ha dado **información incompleta**. Ya el *gestor* se encarga de implementar las decisiones; esa implementación traerá nueva información y el analista vuelve a su trabajo.

La estadística es una ciencia matemática que guía científicamente el análisis de datos. Así el analista trata de seguir una secuencia en su labor:

1.  Apuesta por entender una variable que explicaría un problema de interés (narcotráfico, elecciones, etc). En esta etapa, hace exploración de los datos de esa variable, es decir, organiza los datos en tablas, y de ahi calcula medidas, tablas resumen y gráficos. Se entiende que la variable de interés tiene una variabilidad tal que despierta en el analista la necesidad de preguntar **por qué esa variabilidad**.

2.  Se plantea hipótesis respecto a qué se relaciona con la variabilidad de la variable de interés. Las hipótesis se formula no antes de haber revisado la literatura. En esta etapa hace uso de análisis bivariado o multivariado. Esta etapa se enriquece si está actualizado en las teorías que proponen cierta asociación de la variable de interés con otras variables.

3.  Aplica la técnica estadística que corresponda, tal que confirme, o no, la hipótesis que se planteó.

4.  Interpreta los resultados obtenidos.

5.  Elabora síntesis de lo actuado; propone explicaciones a lo encontrado; y elabora recomendaciones.

Hay muchas opciones para las *técnicas* señaladas en el punto 3. La elección de las mismas dependerá de la preparación del analista. Esta sesión te guiará para que:

1.  Recuerdes cómo y para qué hacer exploración univariada ([ir](#eda))
2.  Recuerdes cómo y para qué hacer análisis bivariada ([ir](#corr)).
3.  Introducir el concepto (y necesidad) de la regresión multivariada ([ir](#rlin)).

<a id='eda'></a>

## I. Explorando la Variable de interés

Supongamos que estás interesado en la "situación de los locales escolares en el Perú". Ese simple interés te lleva a buscar datos para saber tal situación. De pronto te encuentras con estos [datos](https://drive.google.com/drive/folders/1Memoge2Blx3XS1iFlt2hFVJtu9UBF_Xo?usp=sharing) (basado en [CEPLAN](https://www.ceplan.gob.pe/informacion-de-brechas-territoriales/)):

<iframe width="800" height="400" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pubhtml?">

</iframe>

Imaginemos que estos datos son lo 'mejor' que podrás conseguir:

-   Variable: Locales Publicos en buen estado:

    -   Representación: Porcentaje.
    -   Corte: Transversal - sólo 2018.

-   Unidad de Observación: Local Público

-   Unidad de Análisis: Departamento

Teniendo ello en claro, pasemos a explorar los datos:

1.  Carga de datos:

Si los datos están en GoogleDrive, puedes leerlos desde ahí:

```{r}
rm(list = ls()) # limpiar el working environment

linkADrive='https://docs.google.com/spreadsheets/d/e/2PACX-1vS2ZSNM8BIZtoufVTO4Mw3ZmTWW1rAAtsGzFg0shHTJXX-3GmtLsgU-Nqkw5RzDgrNX31GTC9L7LnEz/pub?gid=0&single=true&output=csv'

estadoLocales=read.csv(linkADrive )

head(estadoLocales)
```

La vista preliminar nos muestra varias columnas, pero la de nuestro interés es la última columna de la derecha. A esta altura, antes de explorar los datos de manera estadística, debemos primero verificar cómo R ha intepretado el **tipo** de datos:

```{r}
str(estadoLocales)
```

La columna de interés es de *tipo numérico*, y R también lo tiene intrepretado así.


Nótese que los valores numéricos han sido interpretados como texto. Las comas lo han causado, pero podemos eliminarlas así:

```{r}
gsub(pattern = ",",replacement = "",estadoLocales$peaOcupada)
```

Usemos ese código para volver numérica esa columna:

```{r}
estadoLocales$peaOcupada=gsub(pattern = ",",replacement = "",estadoLocales$peaOcupada)
estadoLocales$peaOcupada=as.numeric(estadoLocales$peaOcupada)

# veamos
str(estadoLocales)
```


2.  Tablas, Gráficos, y Estadígrafos

Si los datos están en el tipo adecuado, puede iniciarse la exploración estadística.

Tenemos 25 observaciones a nivel departamental. La manera más rápida de ver los estadígrafos es usando el comando **summary()**:

```{r}
summary(estadoLocales$buenEstado)
```

Pero, nos faltaría algunos estadígrafos, que podemos añadir así:

```{r, message=FALSE}
library(DescTools)

allStats=c(summary(estadoLocales$buenEstado),
  sd=sd(estadoLocales$buenEstado),
  skew=Skew(estadoLocales$buenEstado),
  kurt=Kurt(estadoLocales$buenEstado),
  cv=CoefVar(estadoLocales$buenEstado))
allStats
```

A esta altura, los gráficos que necesitamos son el histograma:

```{r, message=FALSE}
library(ggplot2)

base=ggplot(data=estadoLocales,
            aes(x=buenEstado))
histogram= base + geom_histogram(aes(y = after_stat(density)),
                 colour = 1, fill = "white",bins=10) +  
    stat_function(fun = dnorm,
                  args = list(mean = allStats['Mean'],
                              sd = allStats['sd']),col='red')
    
histogram
```

Y el boxplot:

```{r}
base=ggplot(data=estadoLocales,
            aes(y=buenEstado))
boxplot=base + geom_boxplot()

boxplot
```

A esta altura sólo falta identificar a los atípicos en el boxplot, lo cual podemos recuperar con *ggplot_build*:
```{r}
valuesFromBox=ggplot_build(boxplot)$data[[1]]
valuesFromBox
```

Los outliers son:
```{r}
outliersLocales=valuesFromBox$outliers[[1]]
outliersLocales
```


Nota que cuando hay outliers, se puede calcular del grafico los maximos y minimos teóricos:

```{r}
valuesFromBox[c('ymin','ymax')]
```

En este caso, sabemos que los valores que exceden 31.4 serán considerados atípicos:

```{r}
estadoLocales[estadoLocales$buenEstado>31.4,]
```


Hasta aquí, podrias sustentar que los valores de la variable buen estado de colegios públicos a nivel regional se distribuyen asimétricamente, con una dispersión baja, y con presencia de valores atípicos cuando los locales en buen estado superan el 31.4 por ciento en los departamentos. Osea, que los locales en buen estados se distribuyen por lo general entre 7.7 y 31.4 por ciento, y que la mitad de los departamentos tienen a lo más un 17.3 por ciento de buenos locales. Las regiones destacadas son sólo dos, pero ninguna de ellas supera siquiera el 50 por ciento.


<a id='corr'></a>

## II. Análisis Bivariado

Consideremos que nos interesa explorar la posible relación entre nuestra variable de interés y la PEA ocupada. Como son *dos* variables de tipo *numérico* la estrategia a seguir es el análisis de correlación. Veamos este **scatterplot**:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
library(ggrepel)
base=ggplot(data=estadoLocales, aes(x=peaOcupada, y=buenEstado))
scatter = base + geom_point()
scatterText = scatter + geom_text_repel(aes(label=DEPARTAMENTO),size=2)
scatterText
```

Calculemos ahora los indices de correlación:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
f1=formula(~peaOcupada + buenEstado)
```

Camino parametrico:

```{r}

pearsonf1=cor.test(f1,data=estadoLocales)[c('estimate','p.value')]
pearsonf1

```

Camino no parametrico

```{r}
spearmanf1=cor.test(f1,data=estadoLocales,method='spearman',exact=F)[c('estimate','p.value')]
spearmanf1
```

Hasta aquí, dudamos que esas variables estén correlacionadas.

Otro caso importante es cuando analizamos nuestra variable versus una variable categórica. Por ejemplo, creemos una columna que indique (de manera imprecisa) costa,sierra y selva:

```{r}
estadoLocales$zona=c('selva','costa','sierra', 'costa','sierra',
                      'sierra','costa','sierra','sierra','selva',
                      'costa','sierra','costa','costa','costa',
                      'selva','selva','costa','sierra','costa',
                      'sierra','selva','costa','costa','selva')

# tendriamos
estadoLocales
```


Pidamos un boxplot, pero por grupos:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
base=ggplot(data=estadoLocales, aes(x=zona, y=buenEstado))
base + geom_boxplot() +  geom_jitter(color="black", size=0.4, alpha=0.9)

```

Parece que hubiese diferencia entre las categorías. Para verificar si hay o no igualdad entre distribuciones depende si las variables se distribuyen o no de manera normal por grupo. Como ello no es fácil de discernir visualmente, complementemos el análisis con los tests de normalidad. Usemos *Shapiro-Wilk* en cada grupo:

```{r, warning=FALSE, message=FALSE, echo=TRUE}


f2=formula(buenEstado~zona)


tablag= aggregate(f2, estadoLocales,
          FUN = function(x) {y <- shapiro.test(x); c(y$statistic, y$p.value)})




shapiroTest=as.data.frame(tablag[,2])
names(shapiroTest)=c("W","Prob")
shapiroTest

```


Habría normalidad en todos los grupo. Usemos entonces tanto la prueba de *Kruskall* (no paramétrica) como la *prueba anova* para analizar las diferencias:

```{r, warning=FALSE, message=FALSE, echo=TRUE}
aovZona=aov(f2,data=estadoLocales)
summary(aovZona)
TukeyHSD(aovZona)


```

```{r}
kruskal.test(f2,data=estadoLocales)
```
```{r}
pairwise.wilcox.test(estadoLocales$buenEstado, estadoLocales$zona, p.adjust.method = "bonferroni")
```

Con toda esta información, iriamos concluyendo también que los locales en buen estado de la costa son más numeros que las otras zonas del país, y que la sierra con la selva podrían estar al mismo nivel.

------------------------------------------------------------------------

<a id='rlin'></a>

## III. Regresión Lineal

La regresión es una técnica donde hay que definir una variable dependiente y una o más independientes. Las independientes pueden tener rol predictor, dependiendo del diseño de investigación, aunque por defecto tiene un rol asociativo.

La regresión sí quiere informar cuánto una variable (*independiente*) está asociada a la variación de otra (*dependiente*), de ahí que es una técnica para probar hipótesis direccionales o asimétricas (las correlaciones tiene hipótesis simétricas).

La regresión devuelve un modelo relacional entre variables, es decir una ecuación, que recoge cómo una o más variables explicarían a otra. Para nuestro caso la variable dependiente es el estado de los locales:

```{r, warning=FALSE, message=FALSE, echo=TRUE}

modelo1=formula(buenEstado~peaOcupada + zona)
```

Por ejemplo, para la hipótesis '*el estado de los locales escolares públicos en una región depende de la PEA ocupada regional y si la ubicación de la región*', la regresión arrojaría este resultado:

<br></br>

```{r, eval=FALSE, echo=FALSE}
regre1=lm(modelo1,data = estadoLocales)
summary(regre1)
```

Hasta aquí, vemos que lo que nos informaba el análisis bivariado se mantiene en la regresión: la PEA no tiene efecto, pero la zona sí (no estar en la costa lo tiene).

Sin embargo, es aquí donde reflexionamos si los datos crudos que tenemos podrían necesitar alguna **transformación**:

```{r}
# la pea como porcentaje
estadoLocales$peaOcu_pct=estadoLocales$peaOcupada/estadoLocales$TOTALpob


modelo2=formula(buenEstado~peaOcu_pct + zona)
regre2=lm(modelo2,data = estadoLocales)
summary(regre2)
```


Aquí aparace algo muy interesante, **primero** que *peaOcu_pct* tiene efecto, pues es *significativo* (*p-valor* es menor que 0.05); **segundo**, que ese efecto es *directo*, pues el coeficiente calculado es positivo (signo de columna *Estimate*); y **tercero** que la *magnitud* de ese efecto es `r round(regre2$coefficients[2],3)`, lo que indica cuánto aumenta, en promedio, la variables dependiente, cuando la variable independiente se incremente en una unidad. Ahora bien, la zona selva ya no difiere de la costa ne esta nueva regresión.

Esto es información suficiente para representar esa relación con una ecuación:

$$  BuenEstado\_pct= `r regre2$coefficients[1]` + `r regre2$coefficients[2]` \cdot PEA\_pct + `r regre2$coefficients[3]` \cdot Populoso + \epsilon$$

El Y verdadero es *BuenEstado_pct*, pero la regresión produce un $\hat{BuenEstado_pct}$ estimado, de ahi la presencia del $\epsilon$. Justamente el *R cuadrado ajustado* (`r summary(regre2)$r.squared`) nos brinda un porcentaje (multiplicalo por 100), que nos da una pista de nuestra cercanía a una situación perfecta (cuando vale **1**).

[al INICIO](#beginning)
